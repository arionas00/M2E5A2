{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIlXmLTGXGFKfAdTV+93xJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arionas00/M2E5A2/blob/main/IDS_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pvRpLI-CqF5O",
        "outputId": "7970712e-16c4-41ce-c026-839f40629f7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Avg_syn_flag  Avg_urg_flag  Avg_fin_flag  Avg_ack_flag  Avg_psh_flag  \\\n",
            "0     -0.230455      4.523595     -1.276876      0.814947     -0.016410   \n",
            "1     -2.088776      2.387204      0.338205      1.419440      0.114833   \n",
            "2     -0.043220      3.578791     -1.073984      0.313930     -1.774397   \n",
            "3      0.065305      3.761012      1.174174      0.761965      0.857883   \n",
            "4      3.875263      1.385111     -2.633112     -0.545981     -1.570862   \n",
            "\n",
            "   Avg_rst_flag  Avg_DNS_pkt  Avg_TCP_pkt  Avg_UDP_pkt  Avg_ICMP_pkt  ...  \\\n",
            "0     -2.995087     1.063889    -2.371085    -2.840079     -0.283463  ...   \n",
            "1      0.913599     0.847367    -0.008734    -1.058475      0.342997  ...   \n",
            "2     -1.006298     0.929811    -3.432328    -1.932374      0.318437  ...   \n",
            "3     -0.597540    -0.022305     1.695764     3.305753      0.792997  ...   \n",
            "4     -3.460744    -1.882090    -2.115882    -2.954608      0.986303  ...   \n",
            "\n",
            "   Min_pkts_lenght  Max_pkts_lenght  StDev_pkts_lenght  Avg_small_payload_pkt  \\\n",
            "0         3.665958        -5.104394           0.690436              -1.584535   \n",
            "1         9.115390        -1.441248           1.710273              -0.274751   \n",
            "2        -0.061857        -1.210975           0.435881               1.494576   \n",
            "3         0.742680        -1.267473          -0.327164               0.188910   \n",
            "4         2.235826        -0.848212          -0.320711              -1.145686   \n",
            "\n",
            "   Avg_payload  Min_payload  Max_payload  StDev_payload  Avg_DNS_over_TCP  \\\n",
            "0    -2.282169    -6.064334    -0.756736      -1.488713          0.694093   \n",
            "1     0.349761     8.602419     0.337548       0.362154          1.193261   \n",
            "2     0.074463    -5.532110    -0.088565       0.702886          0.104589   \n",
            "3    -0.122022     2.977927    -0.472563       1.779231         -0.058810   \n",
            "4     1.356040    -5.251642     0.787477       1.194117          3.840521   \n",
            "\n",
            "   target  \n",
            "0     2.0  \n",
            "1     0.0  \n",
            "2     2.0  \n",
            "3     1.0  \n",
            "4     1.0  \n",
            "\n",
            "[5 rows x 26 columns]\n",
            "Index(['Avg_syn_flag', 'Avg_urg_flag', 'Avg_fin_flag', 'Avg_ack_flag',\n",
            "       'Avg_psh_flag', 'Avg_rst_flag', 'Avg_DNS_pkt', 'Avg_TCP_pkt',\n",
            "       'Avg_UDP_pkt', 'Avg_ICMP_pkt', 'Duration_window_flow', 'Avg_delta_time',\n",
            "       'Min_delta_time', 'Max_delta_time', 'StDev_delta_time',\n",
            "       'Avg_pkts_lenght', 'Min_pkts_lenght', 'Max_pkts_lenght',\n",
            "       'StDev_pkts_lenght', 'Avg_small_payload_pkt', 'Avg_payload',\n",
            "       'Min_payload', 'Max_payload', 'StDev_payload', 'Avg_DNS_over_TCP',\n",
            "       'target'],\n",
            "      dtype='object')\n",
            "(14967, 26)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "index 37 is out of bounds for axis 1 with size 26",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-06331b41b269>\u001b[0m in \u001b[0;36m<cell line: 82>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m# -- LOAD DATA -----------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labelp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labelp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labelp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;31m# to_categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-62-06331b41b269>\u001b[0m in \u001b[0;36mloadDataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# MIN-MAX normalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minx_sel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mdmin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mdmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 37 is out of bounds for axis 1 with size 26"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "An example Intrusion Detection application using Dense, Conv1d and Lstm layers\n",
        "please cite below works if you find it useful:\n",
        "Akgun, Devrim, Selman Hizal, and Unal Cavusoglu. \"A new DDoS attacks intrusion detection\n",
        "model based on deep learning for cybersecurity.\" Computers & Security 118 (2022): 102748.\n",
        "\n",
        "Hizal, Selman, Ünal ÇAVUŞOĞLU, and Devrim AKGÜN. \"A New Deep Learning Based Intrusion\n",
        "Detection System for Cloud Security.\" 2021 3rd International Congress on Human-Computer\n",
        "Interaction, Optimization and Robotic Applications (HORA). IEEE, 2021.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.utils import plot_model\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "import os\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.utils import class_weight\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "epochs = 100\n",
        "nclass = 12\n",
        "\n",
        "def loadDataset():\n",
        "    # Load dataset\n",
        "    filename = 'https://github.com/kdemertzis/EKPA/raw/main/Data/pcap_data.csv'\n",
        "    trainfile = pd.read_csv(filename)\n",
        "    print(trainfile.head())\n",
        "    print(trainfile.columns)\n",
        "    print(trainfile.shape)\n",
        "\n",
        "    data = trainfile.to_numpy()\n",
        "    # Assuming the target column is the last one\n",
        "    target_col_index = -1  # Adjust index based on actual dataset structure\n",
        "    data = data[data[:, target_col_index] != 'DrDoS_LDAP']\n",
        "    np.random.shuffle(data)\n",
        "\n",
        "    label = data[:, target_col_index].astype('str')\n",
        "\n",
        "    # Encoding labels\n",
        "    label[label == 'WebDDoS']       = 0\n",
        "    label[label == 'BENIGN']        = 1\n",
        "    label[label == 'UDP-lag']       = 2\n",
        "    label[label == 'DrDoS_NTP']     = 3\n",
        "    label[label == 'Syn']           = 4\n",
        "    label[label == 'DrDoS_SSDP']    = 5\n",
        "    label[label == 'DrDoS_UDP']     = 6\n",
        "    label[label == 'DrDoS_NetBIOS'] = 7\n",
        "    label[label == 'DrDoS_MSSQL']   = 8\n",
        "    label[label == 'DrDoS_SNMP']    = 9\n",
        "    label[label == 'TFTP']          = 10\n",
        "    label[label == 'DrDoS_DNS']     = 11\n",
        "\n",
        "    # SELECT FEATURES ----------------------------------------------------\n",
        "    inx_sel = -1 + np.array([38, 47, 37, 48, 11, 9, 7, 52, 10, 36, 1, 34, 4, 17, 19, 57, 21,\n",
        "                             18, 22, 24, 32, 50, 23, 55, 51, 5, 3, 39, 40, 43, 58, 12, 25,\n",
        "                             20, 2, 35, 67, 33, 6, 53])\n",
        "\n",
        "    # MIN-MAX normalization\n",
        "    data = data[:, inx_sel]\n",
        "    dmin = data.min(axis=0)\n",
        "    dmax = data.max(axis=0)\n",
        "    data = (data - dmin) / (dmax - dmin)\n",
        "\n",
        "    # Test data 20%\n",
        "    train_data, test_data, train_label, test_label = train_test_split(data, label, test_size=0.20, stratify=label)\n",
        "\n",
        "    # Train 70%, Validation 10%\n",
        "    train_data, val_data, train_label, val_label = train_test_split(train_data, train_label, test_size=0.125, stratify=train_label)\n",
        "\n",
        "    return train_data.astype('float32'), train_label.astype('int32'), val_data.astype('float32'), val_label.astype('int32'), test_data.astype('float32'), test_label.astype('int32')\n",
        "\n",
        "# -- LOAD DATA -----------------------------------------------------------------\n",
        "train_data, train_labelp, val_data, val_labelp, test_data, test_labelp = loadDataset()\n",
        "\n",
        "# to_categorical\n",
        "train_label = to_categorical(train_labelp, nclass)\n",
        "val_label   = to_categorical(val_labelp, nclass)\n",
        "test_label  = to_categorical(test_labelp, nclass)\n",
        "\n",
        "print('train_data.shape=', train_data.shape)\n",
        "print('test_data.shape=', test_data.shape)\n",
        "print('val_data.shape=', val_data.shape)\n",
        "\n",
        "#get the number of features\n",
        "inshape = train_data.shape[1]\n",
        "\n",
        "# Class balancing weights\n",
        "class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(train_labelp), y=train_labelp)\n",
        "class_weights = {i: class_weights[i] for i in range(len(class_weights))}\n",
        "\n",
        "# -- CALLBACKS -----------------------------------------------------------------\n",
        "earlyStopping = EarlyStopping(monitor='val_loss', patience=30, verbose=0, mode='min')\n",
        "\n",
        "modelCheckPoint = ModelCheckpoint('./savemodels/model5class.weights.{epoch:03d}-{val_accuracy:.4f}.hdf5',\n",
        "                                  save_best_only=True, monitor='val_accuracy', mode='max')\n",
        "\n",
        "# -- Baseline models-----------------------------------------------------------\n",
        "# Note: Ensure `models_ddos` is defined or replace with appropriate model definitions.\n",
        "# For example:\n",
        "model = Sequential([\n",
        "    Conv1D(64, 5, activation='relu', input_shape=(inshape, 1)),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(nclass, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# -- TRAIN MODEL --------------------------------------------------------------\n",
        "history = model.fit(train_data, train_label, shuffle=True, epochs=epochs, batch_size=256,\n",
        "                    validation_data=(val_data, val_label), callbacks=[modelCheckPoint],\n",
        "                    class_weight=class_weights, workers=3)\n",
        "\n",
        "# -- Load best model ----------------------------------------------------------\n",
        "str_models = os.listdir('./savemodels')\n",
        "str_models = np.sort(str_models)\n",
        "best_model = str_models[-1]\n",
        "print('best_model=', best_model)\n",
        "model.load_weights('./savemodels/'+best_model)\n",
        "\n",
        "# --Confusion matrix ----------------------------------------------------------\n",
        "print('TEST DATA-Confusion matrix:')\n",
        "pred = model.predict(test_data)\n",
        "pred_y = pred.argmax(axis=-1)\n",
        "\n",
        "cm = confusion_matrix(test_labelp.astype('int32'), pred_y)\n",
        "print(cm)\n",
        "\n",
        "print('Accuracy ratios for each class')\n",
        "for i, label in enumerate([\"WebDDoS\", \"BENIGN\", \"UDP-lag\", \"DrDoS_NTP\", \"Syn\", \"DrDoS_SSDP\", \"DrDoS_UDP\", \"DrDoS_NetBIOS\", \"DrDoS_MSSQL\", \"DrDoS_SNMP\", \"TFTP\", \"DrDoS_DNS\"]):\n",
        "    print(f'{label} =', cm[i, i] / np.sum(cm[i, :]))\n",
        "\n",
        "# -- Confusion matrix plot\n",
        "cmo = ConfusionMatrixDisplay(cm, display_labels=[\"WebDDoS\", \"BENIGN\", \"UDP-lag\", \"DrDoS_NTP\", \"Syn\", \"DrDoS_SSDP\", \"DrDoS_UDP\", \"DrDoS_NetBIOS\", \"DrDoS_MSSQL\", \"DrDoS_SNMP\", \"TFTP\", \"DrDoS_DNS\"])\n",
        "fig, ax = plt.subplots(figsize=(12,12))\n",
        "cmo.plot(ax=ax, xticks_rotation=45)\n",
        "\n",
        "# Plot training and validation accuracy and loss graphs\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "np.save('historydata.npy', [acc, val_acc, loss, val_loss])\n",
        "[acc, val_acc, loss, val_loss] = np.load('historydata.npy', allow_pickle=True)\n",
        "\n",
        "plt.figure()\n",
        "epochs = range(len(acc))\n",
        "plt.plot(epochs, acc, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'r.', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r.', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "source": [],
      "cell_type": "code",
      "metadata": {
        "id": "VyBhM9pcxI8v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}